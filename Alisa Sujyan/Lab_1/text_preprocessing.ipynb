{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uzlEwLeBFMj3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial\n",
        "intelligence concerned with the interactions between computers and human language. It's used to\n",
        "analyze text, allowing machines to understand, interpret, and manipulate human language. NLP has\n",
        "many real-world applications, including machine translation, sentiment analysis, and chatbots.\"\"\""
      ],
      "metadata": {
        "id": "qUb2_XI4D01g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = \"\"\"Yesterday, the children were running faster than the dogs and cats in the park. She had bought better shoes for the race,\n",
        "but the mice were seeing the geese near the pond. They went to the store because they did not have enough food. The cars were parked\n",
        "outside while the feet of many people were tired after walking all day. Although she was going to the market, she did not see the fastest\n",
        "runner or the worst player in the game. The cats were chasing the mice, and the children were watching them carefully.\"\"\""
      ],
      "metadata": {
        "id": "getvvq_9TePZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization: Split the text into individual words (tokens)."
      ],
      "metadata": {
        "id": "kXfAMh2dDyDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CGRQaqaDlDe",
        "outputId": "4625fdf9-dce1-4aa3-9283-35eb6b8f4011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yesterday,', 'the', 'children', 'were', 'running', 'faster', 'than', 'the', 'dogs', 'and', 'cats', 'in', 'the', 'park.', 'She', 'had', 'bought', 'better', 'shoes', 'for', 'the', 'race,', 'but', 'the', 'mice', 'were', 'seeing', 'the', 'geese', 'near', 'the', 'pond.', 'They', 'went', 'to', 'the', 'store', 'because', 'they', 'did', 'not', 'have', 'enough', 'food.', 'The', 'cars', 'were', 'parked', 'outside', 'while', 'the', 'feet', 'of', 'many', 'people', 'were', 'tired', 'after', 'walking', 'all', 'day.', 'Although', 'she', 'was', 'going', 'to', 'the', 'market,', 'she', 'did', 'not', 'see', 'the', 'fastest', 'runner', 'or', 'the', 'worst', 'player', 'in', 'the', 'game.', 'The', 'cats', 'were', 'chasing', 'the', 'mice,', 'and', 'the', 'children', 'were', 'watching', 'them', 'carefully.']\n"
          ]
        }
      ],
      "source": [
        "tokens = data.split()\n",
        "print(tokens)\n",
        "\n",
        "# tokens = new_data.split()\n",
        "# print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lowercasing: Convert all tokens to lowercase.\n"
      ],
      "metadata": {
        "id": "GqKhG_mQFTje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lowercased_tokens = [token.lower() for token in tokens]\n",
        "print(lowercased_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YgQY797FW3v",
        "outputId": "4ffcfae9-b0c9-477f-897d-5b022a0e6c52"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yesterday,', 'the', 'children', 'were', 'running', 'faster', 'than', 'the', 'dogs', 'and', 'cats', 'in', 'the', 'park.', 'she', 'had', 'bought', 'better', 'shoes', 'for', 'the', 'race,', 'but', 'the', 'mice', 'were', 'seeing', 'the', 'geese', 'near', 'the', 'pond.', 'they', 'went', 'to', 'the', 'store', 'because', 'they', 'did', 'not', 'have', 'enough', 'food.', 'the', 'cars', 'were', 'parked', 'outside', 'while', 'the', 'feet', 'of', 'many', 'people', 'were', 'tired', 'after', 'walking', 'all', 'day.', 'although', 'she', 'was', 'going', 'to', 'the', 'market,', 'she', 'did', 'not', 'see', 'the', 'fastest', 'runner', 'or', 'the', 'worst', 'player', 'in', 'the', 'game.', 'the', 'cats', 'were', 'chasing', 'the', 'mice,', 'and', 'the', 'children', 'were', 'watching', 'them', 'carefully.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Punctuation Removal: Remove all punctuation marks from the tokens."
      ],
      "metadata": {
        "id": "9BpjU1lUFuku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = ',.?!:()\\'\\\";'\n",
        "cleaned_tokens = [token.strip(punctuation) for token in lowercased_tokens]\n",
        "print(cleaned_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXNMC0JnFvRO",
        "outputId": "01ed9a60-f3fc-4e01-d5ba-4f6e754726f5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yesterday', 'the', 'children', 'were', 'running', 'faster', 'than', 'the', 'dogs', 'and', 'cats', 'in', 'the', 'park', 'she', 'had', 'bought', 'better', 'shoes', 'for', 'the', 'race', 'but', 'the', 'mice', 'were', 'seeing', 'the', 'geese', 'near', 'the', 'pond', 'they', 'went', 'to', 'the', 'store', 'because', 'they', 'did', 'not', 'have', 'enough', 'food', 'the', 'cars', 'were', 'parked', 'outside', 'while', 'the', 'feet', 'of', 'many', 'people', 'were', 'tired', 'after', 'walking', 'all', 'day', 'although', 'she', 'was', 'going', 'to', 'the', 'market', 'she', 'did', 'not', 'see', 'the', 'fastest', 'runner', 'or', 'the', 'worst', 'player', 'in', 'the', 'game', 'the', 'cats', 'were', 'chasing', 'the', 'mice', 'and', 'the', 'children', 'were', 'watching', 'them', 'carefully']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Stop Word Removal: Remove common stop words (e.g., \"the\", \"is\", \"and\")."
      ],
      "metadata": {
        "id": "ZRzNDTfrKJkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"for\", \"to\", \"of\",\n",
        "\"and\", \"is\", \"are\"]\n",
        "\n",
        "filtered_tokens = [token for token in cleaned_tokens if token not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIVfSVagKOSF",
        "outputId": "fb75b186-ee0d-454c-e0ed-d4a97320792b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yesterday', 'children', 'were', 'running', 'faster', 'than', 'dogs', 'cats', 'park', 'she', 'had', 'bought', 'better', 'shoes', 'race', 'but', 'mice', 'were', 'seeing', 'geese', 'near', 'pond', 'they', 'went', 'store', 'because', 'they', 'did', 'not', 'have', 'enough', 'food', 'cars', 'were', 'parked', 'outside', 'while', 'feet', 'many', 'people', 'were', 'tired', 'after', 'walking', 'all', 'day', 'although', 'she', 'was', 'going', 'market', 'she', 'did', 'not', 'see', 'fastest', 'runner', 'or', 'worst', 'player', 'game', 'cats', 'were', 'chasing', 'mice', 'children', 'were', 'watching', 'them', 'carefully']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a simple lemmatization function that uses a dictionary to map common words to their base\n",
        "\n"
      ],
      "metadata": {
        "id": "octtAtQPNL8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import final\n",
        "dict_lemmatization = {\n",
        "    \"is\": \"be\",\n",
        "    \"are\": \"be\",\n",
        "    \"was\": \"be\",\n",
        "    \"were\": \"be\",\n",
        "    \"has\": \"have\",\n",
        "    \"had\": \"have\",\n",
        "    \"does\": \"do\",\n",
        "    \"did\": \"do\",\n",
        "    \"better\": \"good\",\n",
        "    \"best\": \"good\",\n",
        "    \"worse\": \"bad\",\n",
        "    \"worst\": \"bad\",\n",
        "    \"it's\": \"it is\", # [\"it\", \"is\"]\n",
        "    \"used\" : \"use\",\n",
        "    \"chatbots\": \"chatbot\",\n",
        "    \"computers\": \"computer\",\n",
        "    \"interactions\": \"interaction\",\n",
        "    \"linguistics\": \"linguistic\",\n",
        "}\n",
        "\n",
        "def lemmatization(token):\n",
        "    return dict_lemmatization.get(token, token)\n",
        "\n",
        "lemmatized_tokens = [lemmatization(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyxr3H53Nejb",
        "outputId": "c2e44282-02fd-43f2-e6d9-6747e65ad561"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yesterday', 'children', 'be', 'running', 'faster', 'than', 'dogs', 'cats', 'park', 'she', 'have', 'bought', 'good', 'shoes', 'race', 'but', 'mice', 'be', 'seeing', 'geese', 'near', 'pond', 'they', 'went', 'store', 'because', 'they', 'do', 'not', 'have', 'enough', 'food', 'cars', 'be', 'parked', 'outside', 'while', 'feet', 'many', 'people', 'be', 'tired', 'after', 'walking', 'all', 'day', 'although', 'she', 'be', 'going', 'market', 'she', 'do', 'not', 'see', 'fastest', 'runner', 'or', 'bad', 'player', 'game', 'cats', 'be', 'chasing', 'mice', 'children', 'be', 'watching', 'them', 'carefully']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_HE5ZwYiS70Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Stemming: Reduce words to their root form using a simple algorithm. ('ing', 'ed')"
      ],
      "metadata": {
        "id": "pzBRYlEDRiwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmer(token):\n",
        "    if token.endswith('ing'):\n",
        "        return token[:-3]\n",
        "    elif token.endswith('ed'):\n",
        "        return token[:-2]\n",
        "    else:\n",
        "        return token\n",
        "\n",
        "stemmed_tokens = [stemmer(token) for token in lemmatized_tokens]\n",
        "print(stemmed_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RcgW5-aRiKC",
        "outputId": "d5a07716-442c-43db-99a4-cb1b673a4664"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yesterday', 'children', 'be', 'runn', 'faster', 'than', 'dogs', 'cats', 'park', 'she', 'have', 'bought', 'good', 'shoes', 'race', 'but', 'mice', 'be', 'see', 'geese', 'near', 'pond', 'they', 'went', 'store', 'because', 'they', 'do', 'not', 'have', 'enough', 'food', 'cars', 'be', 'park', 'outside', 'while', 'feet', 'many', 'people', 'be', 'tir', 'after', 'walk', 'all', 'day', 'although', 'she', 'be', 'go', 'market', 'she', 'do', 'not', 'see', 'fastest', 'runner', 'or', 'bad', 'player', 'game', 'cats', 'be', 'chas', 'mice', 'children', 'be', 'watch', 'them', 'carefully']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x0up7ruoLK0E"
      }
    }
  ]
}