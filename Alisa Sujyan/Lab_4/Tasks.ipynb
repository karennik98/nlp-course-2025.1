{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.\tData Preprocessing",
   "id": "46e3b4237d6ce395"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:30.384073Z",
     "start_time": "2025-09-27T16:53:30.324282Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\MSI GF66\\PycharmProjects\\NLP\\BBC_text\\bbc-text.csv\")\n",
    "\n",
    "print(data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category                                               text\n",
      "0           tech  tv future in the hands of viewers with home th...\n",
      "1       business  worldcom boss  left books alone  former worldc...\n",
      "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3          sport  yeading face newcastle in fa cup premiership s...\n",
      "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:30.391371Z",
     "start_time": "2025-09-27T16:53:30.388471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import re"
   ],
   "id": "d798c25c82fdb2f",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:30.422009Z",
     "start_time": "2025-09-27T16:53:30.418379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special_characters(t):\n",
    "    if isinstance(t, str):\n",
    "        t = re.sub(r\"[.,!@?#$%&*()+=\\-_{}\\[\\];:'\\\"/\\\\|<>`~]\", \"\", t)\n",
    "        return t\n",
    "    return t"
   ],
   "id": "6a22a5193f552426",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:30.440775Z",
     "start_time": "2025-09-27T16:53:30.428012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(t):\n",
    "    if isinstance(t, str):\n",
    "        tokens = t.split()\n",
    "        new = [token for token in tokens if token not in stopwords]\n",
    "        return \" \".join(new)\n",
    "    return t"
   ],
   "id": "a086cc99fd828ef8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n",
      "[nltk_data]     GF66\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:30.458790Z",
     "start_time": "2025-09-27T16:53:30.455642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_cleaning(dataset):\n",
    "    dataset = dataset[\"text\"].str.lower()\n",
    "    dataset = dataset.apply(remove_special_characters)\n",
    "    dataset = dataset.apply(remove_stop_words)\n",
    "    return dataset"
   ],
   "id": "31dd9b2885a7401b",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.771676Z",
     "start_time": "2025-09-27T16:53:30.474504Z"
    }
   },
   "cell_type": "code",
   "source": "data = data_cleaning(data)",
   "id": "480dbb7bbb62e12",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.789418Z",
     "start_time": "2025-09-27T16:53:31.785918Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.head())",
   "id": "cab0d307a9c22808",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tv future hands viewers home theatre systems p...\n",
      "1    worldcom boss left books alone former worldcom...\n",
      "2    tigers wary farrell gamble leicester say rushe...\n",
      "3    yeading face newcastle fa cup premiership side...\n",
      "4    ocean twelve raids box office ocean twelve cri...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.877492Z",
     "start_time": "2025-09-27T16:53:31.872515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(t):\n",
    "    if isinstance(t, str):\n",
    "        return t.split()\n",
    "    return []"
   ],
   "id": "621d1d61193a2748",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.929408Z",
     "start_time": "2025-09-27T16:53:31.894474Z"
    }
   },
   "cell_type": "code",
   "source": "data_tokens = data.apply(tokenize)\n",
   "id": "7c3d469de628e3cc",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.947369Z",
     "start_time": "2025-09-27T16:53:31.943932Z"
    }
   },
   "cell_type": "code",
   "source": "print(data_tokens[4])",
   "id": "559c1ee365f84975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ocean', 'twelve', 'raids', 'box', 'office', 'ocean', 'twelve', 'crime', 'caper', 'sequel', 'starring', 'george', 'clooney', 'brad', 'pitt', 'julia', 'roberts', 'gone', 'straight', 'number', 'one', 'us', 'box', 'office', 'chart', 'took', '408m', '£21m', 'weekend', 'ticket', 'sales', 'according', 'studio', 'estimates', 'sequel', 'follows', 'master', 'criminals', 'try', 'pull', 'three', 'major', 'heists', 'across', 'europe', 'knocked', 'last', 'week', 'number', 'one', 'national', 'treasure', 'third', 'place', 'wesley', 'snipes', 'blade', 'trinity', 'second', 'taking', '161m', '£84m', 'rounding', 'top', 'five', 'animated', 'fable', 'polar', 'express', 'starring', 'tom', 'hanks', 'festive', 'comedy', 'christmas', 'kranks', 'ocean', 'twelve', 'box', 'office', 'triumph', 'marks', 'fourthbiggest', 'opening', 'december', 'release', 'us', 'three', 'films', 'lord', 'rings', 'trilogy', 'sequel', 'narrowly', 'beat', '2001', 'predecessor', 'ocean', 'eleven', 'took', '381m', '£198m', 'opening', 'weekend', '184m', '£958m', 'total', 'remake', '1960s', 'film', 'starring', 'frank', 'sinatra', 'rat', 'pack', 'ocean', 'eleven', 'directed', 'oscarwinning', 'director', 'steven', 'soderbergh', 'soderbergh', 'returns', 'direct', 'hit', 'sequel', 'reunites', 'clooney', 'pitt', 'roberts', 'matt', 'damon', 'andy', 'garcia', 'elliott', 'gould', 'catherine', 'zetajones', 'joins', 'allstar', 'cast', 'fun', 'good', 'holiday', 'movie', 'said', 'dan', 'fellman', 'president', 'distribution', 'warner', 'bros', 'however', 'us', 'critics', 'less', 'complimentary', '110m', '£572m', 'project', 'los', 'angeles', 'times', 'labelling', 'dispiriting', 'vanity', 'project', 'milder', 'review', 'new', 'york', 'times', 'dubbed', 'sequel', 'unabashedly', 'trivial']\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.\tImplement Bag of Words",
   "id": "c1fdb0c2becb2992"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.974088Z",
     "start_time": "2025-09-27T16:53:31.971308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_vocab(tokenized_data):\n",
    "\n",
    "    vocab = set()\n",
    "    for tokens in tokenized_data:\n",
    "        for token in tokens:\n",
    "            if token.isalpha():\n",
    "                vocab.add(token)\n",
    "    return sorted(list(vocab))"
   ],
   "id": "4d40866c0fbbf395",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:31.991289Z",
     "start_time": "2025-09-27T16:53:31.987798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def document_into_bow(document_tokens, vocabulary):\n",
    "    bow = [0 for i in range(len(vocabulary))]\n",
    "    word_tokens = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "    for token in document_tokens:\n",
    "        if token in word_tokens:\n",
    "            bow[word_tokens[token]] += 1\n",
    "    return bow"
   ],
   "id": "98c5b8692b2692a6",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:40.000729Z",
     "start_time": "2025-09-27T16:53:32.006187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = int(len(data_tokens) * 0.8)\n",
    "train, test = data_tokens[:m], data_tokens[m:]\n",
    "\n",
    "vocabulary = build_vocab(train)\n",
    "\n",
    "training_bow = []\n",
    "for doc in train:\n",
    "    training_bow.append(document_into_bow(doc, vocabulary))\n",
    "\n",
    "test_bow = []\n",
    "for doc in test:\n",
    "    test_bow.append(document_into_bow(doc, vocabulary))\n",
    "\n",
    "print(\"Vocabulary size: \", len(vocabulary))"
   ],
   "id": "3962b456f0b158d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  27554\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.\tImplement TF-IDF",
   "id": "91f61e62e3f4463d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:40.086085Z",
     "start_time": "2025-09-27T16:53:40.081902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_tf(document_tokens, vocabulary):\n",
    "    tf_vector = []\n",
    "    document_len = len(document_tokens)\n",
    "\n",
    "    word_counts = {}\n",
    "    for word in document_tokens:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    for word in vocabulary:\n",
    "        count = word_counts.get(word, 0)\n",
    "        tf = count / document_len if document_len > 0 else 0\n",
    "        tf_vector.append(tf)\n",
    "\n",
    "    return tf_vector"
   ],
   "id": "f9fb4a0e89c7d7a7",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:40.103886Z",
     "start_time": "2025-09-27T16:53:40.100685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(all_docs, vocabulary):\n",
    "    N = len(all_docs)\n",
    "    idf_vector = []\n",
    "\n",
    "    for word in vocabulary:\n",
    "        df = 0\n",
    "        for doc in all_docs:\n",
    "            if word in doc:\n",
    "                df += 1\n",
    "        idf = math.log(N / (df + 1)) + 1\n",
    "        idf_vector.append(idf)\n",
    "\n",
    "    return idf_vector"
   ],
   "id": "61c5fe90fa65087b",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:53:40.121187Z",
     "start_time": "2025-09-27T16:53:40.118342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_tfidf(doc_tokens, vocabulary, idf_vector):\n",
    "    tf_vector = compute_tf(doc_tokens, vocabulary)\n",
    "    tf_idf_vector = []\n",
    "\n",
    "    for tf, idf in zip(tf_vector, idf_vector):\n",
    "        tf_idf_vector.append(tf * idf)\n",
    "    return tf_idf_vector"
   ],
   "id": "5bc484606a2fe789",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:55:51.284191Z",
     "start_time": "2025-09-27T16:53:40.136197Z"
    }
   },
   "cell_type": "code",
   "source": "idf_vector = compute_idf(data_tokens, vocabulary)",
   "id": "b3b3a0b841832820",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:56:03.103467Z",
     "start_time": "2025-09-27T16:55:51.312360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tf_idf = []\n",
    "for doc in train:\n",
    "    train_tf_idf.append(compute_tfidf(doc, vocabulary, idf_vector))\n",
    "\n",
    "test_tf_idf = []\n",
    "for doc in test:\n",
    "    test_tf_idf.append(compute_tfidf(doc, vocabulary, idf_vector))\n"
   ],
   "id": "c97915f8409a381b",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:56:03.112997Z",
     "start_time": "2025-09-27T16:56:03.109472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"First 10 words:\", vocabulary[:30])\n",
    "print(\"First training doc TF-IDF vector sample:\", train_tf_idf[0][:50], \"...\")"
   ],
   "id": "18151e03c5d840e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27554\n",
      "First 10 words: ['aa', 'aaa', 'aaas', 'aac', 'aaliyah', 'aaltra', 'aamir', 'aan', 'aara', 'aarhus', 'aaron', 'abacus', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abate', 'abating', 'abba', 'abbas', 'abbasi', 'abbey', 'abbot', 'abbott', 'abbreviated', 'abc', 'abd', 'abdellatif', 'abdication', 'abdominal']\n",
      "First training doc TF-IDF vector sample: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018558292453493247, 0.0, 0.0, 0.0, 0.0, 0.0] ...\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.\tAnalysis\n",
   "id": "9a1ab10bc7372937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:56:03.240330Z",
     "start_time": "2025-09-27T16:56:03.235977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_words_by_category(tfidf_vector, labels, vocabulary, top_n=10):\n",
    "    categories = set(labels)\n",
    "    results = {}\n",
    "\n",
    "    for category in categories:\n",
    "        indices = [i for i, label in enumerate(labels) if label == category]\n",
    "\n",
    "        category_vectors = [tfidf_vector[i] for i in indices]\n",
    "\n",
    "        avg_vector = np.mean(category_vectors, axis=0)\n",
    "\n",
    "        top_indices = np.argsort(avg_vector)[-top_n:][::-1]\n",
    "        top_words = [(vocabulary[i], avg_vector[i]) for i in top_indices]\n",
    "\n",
    "        results[category] = top_words\n",
    "\n",
    "    return results"
   ],
   "id": "d59bc16fe42f8864",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:56:05.065861Z",
     "start_time": "2025-09-27T16:56:03.258033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\MSI GF66\\PycharmProjects\\NLP\\BBC_text\\bbc-text.csv\")\n",
    "\n",
    "train_labels = list(data['category'][:len(train_tf_idf)])\n",
    "category_top_words = top_words_by_category(train_tf_idf, train_labels, vocabulary, top_n=10)\n",
    "\n",
    "for category, words in category_top_words.items():\n",
    "    print(f\"\\nCategory {category}:\")\n",
    "    for word, score in words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ],
   "id": "fe8f39701b290b89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category business:\n",
      "said: 0.0192\n",
      "us: 0.0153\n",
      "growth: 0.0130\n",
      "company: 0.0128\n",
      "economy: 0.0126\n",
      "bank: 0.0122\n",
      "year: 0.0120\n",
      "mr: 0.0116\n",
      "market: 0.0114\n",
      "oil: 0.0113\n",
      "\n",
      "Category entertainment:\n",
      "film: 0.0348\n",
      "best: 0.0196\n",
      "show: 0.0151\n",
      "music: 0.0144\n",
      "said: 0.0141\n",
      "awards: 0.0126\n",
      "festival: 0.0126\n",
      "band: 0.0122\n",
      "award: 0.0119\n",
      "star: 0.0115\n",
      "\n",
      "Category politics:\n",
      "mr: 0.0318\n",
      "said: 0.0252\n",
      "labour: 0.0244\n",
      "election: 0.0195\n",
      "party: 0.0195\n",
      "blair: 0.0178\n",
      "government: 0.0170\n",
      "would: 0.0167\n",
      "brown: 0.0136\n",
      "minister: 0.0120\n",
      "\n",
      "Category sport:\n",
      "england: 0.0145\n",
      "game: 0.0144\n",
      "said: 0.0137\n",
      "win: 0.0132\n",
      "cup: 0.0119\n",
      "chelsea: 0.0113\n",
      "match: 0.0112\n",
      "club: 0.0107\n",
      "play: 0.0104\n",
      "injury: 0.0104\n",
      "\n",
      "Category tech:\n",
      "said: 0.0165\n",
      "people: 0.0161\n",
      "users: 0.0145\n",
      "software: 0.0137\n",
      "microsoft: 0.0128\n",
      "technology: 0.0125\n",
      "net: 0.0123\n",
      "mobile: 0.0120\n",
      "broadband: 0.0118\n",
      "digital: 0.0113\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T16:56:05.185468Z",
     "start_time": "2025-09-27T16:56:05.140200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_vector = compute_tf(data_tokens[0], vocabulary)\n",
    "idf_vector_np = np.array(idf_vector)\n",
    "\n",
    "tf_np = np.array(tf_vector)\n",
    "\n",
    "N = 750\n",
    "\n",
    "top_tf = np.argsort(tf_np)[-N:][::-1]\n",
    "top_idf = np.argsort(idf_vector_np)[-N:][::-1]\n",
    "\n",
    "bottom_tf = np.argsort(tf_np)[:N]\n",
    "bottom_idf = np.argsort(idf_vector_np)[:N]\n",
    "\n",
    "high_tf_low_idf = [i for i in top_tf if i in bottom_idf]\n",
    "low_tf_high_idf = [i for i in top_idf if i in bottom_tf]\n",
    "\n",
    "high_tf_low_idf_words = [vocabulary[i] for i in high_tf_low_idf]\n",
    "low_tf_high_idf_words = [vocabulary[i] for i in low_tf_high_idf]\n",
    "\n",
    "print(\"High TF, Low IDF: \", high_tf_low_idf_words)\n",
    "print(\"Low TF, High IDF: \", low_tf_high_idf_words)"
   ],
   "id": "b28f64202b9002b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High TF, Low IDF:  ['tv', 'people', 'want', 'us', 'content', 'means', 'one', 'new', 'said', 'show', 'bbc', 'launched', 'much', 'companies', 'uk', 'also', 'play', 'like', 'forward', 'choice', 'instead', 'everyone', 'way', 'might', 'consumer', 'future', 'find', 'years', 'technology', 'network', 'video', 'many', 'home', 'europe', 'take', 'mr', 'moment', 'digital', 'time', 'terms', 'used', 'leading', 'website', 'set', 'group', 'everything', 'experience', 'firm', 'even', 'biggest', 'mobile', 'record', 'taking', 'big', 'know', 'news', 'different', 'added', 'although', 'possible', 'growing', 'japan', 'bill', 'system', 'months', 'could', 'business', 'hard', 'help', 'programme', 'work', 'issue', 'issues', 'impact', 'available', 'well', 'today', 'together', 'senior', 'see', 'put', 'announced', 'example', 'hours', 'five', 'told', 'annual', 'called', 'end', 'rather', 'getting', 'important', 'yet', 'personal', 'already', 'suggested', 'lost', 'market', 'service', 'services', 'according', 'control', 'president', 'via', 'front', 'chief', 'particularly', 'allow', 'challenge', 'firms', 'first', 'followed', 'focus', 'fans', 'far', 'failed', 'fact', 'family']\n",
      "Low TF, High IDF:  ['aa', 'pounder', 'pounded', 'poulton', 'potts', 'pots', 'powderject']\n"
     ]
    }
   ],
   "execution_count": 131
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
