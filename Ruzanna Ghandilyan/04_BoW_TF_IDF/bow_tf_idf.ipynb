{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "kQp9WBox4IG0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9c9b789-6b4c-4cb5-d000-7d04b4f18753",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:10.458555Z",
     "start_time": "2025-09-29T11:35:08.124910Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruzgh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TmxL55At7KZ0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc38a1f8-1e7d-4b79-f604-5c0e360639e6",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:10.631459Z",
     "start_time": "2025-09-29T11:35:10.469560Z"
    }
   },
   "source": [
    "data = pd.read_csv(\"bbc-text.csv\")\n",
    "print(data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category                                               text\n",
      "0           tech  tv future in the hands of viewers with home th...\n",
      "1       business  worldcom boss  left books alone  former worldc...\n",
      "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3          sport  yeading face newcastle in fa cup premiership s...\n",
      "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing:\n",
    "1. Clean the text data by removing punctuation, converting to lowercase, and removing stop words.\n",
    "2. Tokenize the text into individual words.\n"
   ],
   "metadata": {
    "id": "b84ZIaTXaJWJ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yiGiFUsA7xdy",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:11.800163Z",
     "start_time": "2025-09-29T11:35:11.606673Z"
    }
   },
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove symbols\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data[\"tokens\"] = data[\"text\"].apply(preprocess)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7MNcH--o76fQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "outputId": "924d0eb9-2cda-4110-fe1a-406c40c83e36",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:11.823624Z",
     "start_time": "2025-09-29T11:35:11.804691Z"
    }
   },
   "source": [
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1     [worldcom, boss, left, books, alone, former, w...  \n",
       "2     [tigers, wary, farrell, gamble, leicester, say...  \n",
       "3     [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4     [ocean, twelve, raids, box, office, ocean, twe...  \n",
       "...                                                 ...  \n",
       "2220  [cars, pull, us, retail, figures, us, retail, ...  \n",
       "2221  [kilroy, unveils, immigration, policy, ex, cha...  \n",
       "2222  [rem, announce, new, glasgow, concert, us, ban...  \n",
       "2223  [political, squabbles, snowball, become, commo...  \n",
       "2224  [souness, delight, euro, progress, boss, graem...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>[cars, pull, us, retail, figures, us, retail, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>[kilroy, unveils, immigration, policy, ex, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>[rem, announce, new, glasgow, concert, us, ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>[political, squabbles, snowball, become, commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>[souness, delight, euro, progress, boss, graem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lwfWOKrR8uUa",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:12.013820Z",
     "start_time": "2025-09-29T11:35:11.963314Z"
    }
   },
   "source": [
    "# Build vocab\n",
    "vocab_set = set()\n",
    "for doc in data[\"tokens\"]:\n",
    "    vocab_set.update(doc)\n",
    "\n",
    "sorted_vocab = sorted(vocab_set)\n",
    "vocab = {word: i for i, word in enumerate(sorted_vocab)}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "howCBedw_UKz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e5f029b-25cb-4a0e-c721-36145cc68720",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:12.098207Z",
     "start_time": "2025-09-29T11:35:12.088698Z"
    }
   },
   "source": [
    "list(vocab.items())[:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aa', 0),\n",
       " ('aaa', 1),\n",
       " ('aaas', 2),\n",
       " ('aac', 3),\n",
       " ('aadc', 4),\n",
       " ('aaliyah', 5),\n",
       " ('aaltra', 6),\n",
       " ('aamir', 7),\n",
       " ('aan', 8),\n",
       " ('aara', 9)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Uha5YvX_Uyn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "922afa70-4866-4328-db96-72406fff6a7c",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:12.195116Z",
     "start_time": "2025-09-29T11:35:12.190628Z"
    }
   },
   "source": [
    "len(vocab)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27758"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3_D2yLZ_3au"
   },
   "source": [
    "# Implement Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y9-Z3NPi9jVo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f0525ba-e84f-4a39-893c-0dd0a288085c",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:14.754502Z",
     "start_time": "2025-09-29T11:35:12.281147Z"
    }
   },
   "source": [
    "# docs to BoW\n",
    "X_bow = []\n",
    "for doc in data[\"tokens\"]:\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in doc:\n",
    "        if token in vocab:\n",
    "            vector[vocab[token]] += 1\n",
    "    X_bow.append(vector)\n",
    "\n",
    "X_bow = np.array(X_bow)\n",
    "X_bow"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(2225, 27758))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bU9woBaY_oCH",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:14.782760Z",
     "start_time": "2025-09-29T11:35:14.779759Z"
    }
   },
   "source": [
    "y = data[\"category\"]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD_oyDZ5__Pe"
   },
   "source": [
    "# Implement TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nf-phLzJAjL5",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:14.899294Z",
     "start_time": "2025-09-29T11:35:14.896271Z"
    }
   },
   "source": [
    "# IDF\n",
    "N = len(data[\"tokens\"])\n",
    "idf = {}\n",
    "\n",
    "all_tokens = set()\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "63HNzVAMDpBb",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:35:14.980258Z",
     "start_time": "2025-09-29T11:35:14.925770Z"
    }
   },
   "source": [
    "for doc in data[\"tokens\"]:\n",
    "    for word in doc:\n",
    "        all_tokens.add(word)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6v6XTP5LDqDN",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:38:36.135136Z",
     "start_time": "2025-09-29T11:35:15.071803Z"
    }
   },
   "source": [
    "for word in all_tokens:\n",
    "    df = sum(1 for doc in data[\"tokens\"] if word in doc)\n",
    "    idf[word] = math.log(N / (1 + df))\n",
    "\n",
    "idf_array = np.array([idf[word] for word in vocab.keys()])\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H-4ebFFyA0m5",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:38:44.934611Z",
     "start_time": "2025-09-29T11:38:36.744979Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "X_tfidf = []  # TF-IDF vectors\n",
    "\n",
    "for tokens in data[\"tokens\"]:\n",
    "    counts = Counter(tokens)            # word frequencies\n",
    "    total_terms = len(tokens)           # total terms in the document\n",
    "    vector = [0] * len(vocab)\n",
    "\n",
    "    for word, count in counts.items():\n",
    "        if word in vocab:\n",
    "            tf = count / total_terms    # tf\n",
    "            vector[vocab[word]] = tf * idf[word]  # TF-IDF\n",
    "\n",
    "    X_tfidf.append(vector)\n",
    "\n",
    "X_tfidf = np.array(X_tfidf)\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis:\n",
    "1. For a given category, find the top 10 words with the highest average TF-IDF scores.\n",
    "2. Identify words that have high TF scores but low IDF scores, and vice versa.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "XoX-rxbeZ-Fq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "categories = set(y)\n",
    "\n",
    "category_avg_tfidf = {}"
   ],
   "metadata": {
    "id": "bNsGh95Iax7z",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:38:45.031812Z",
     "start_time": "2025-09-29T11:38:45.025836Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "# 1\n",
    "for category in categories:\n",
    "\n",
    "    avg_tfidf = X_tfidf[[i for i, label in enumerate(y) if label == category]].mean(axis=0)\n",
    "    word_avg_tfidf = dict(zip(vocab, avg_tfidf))\n",
    "\n",
    "    top_words = sorted(word_avg_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    category_avg_tfidf[category] = top_words\n",
    "\n",
    "for category, top_words in category_avg_tfidf.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for word, score in top_words:\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ],
   "metadata": {
    "id": "9sLLMWlRa7Ii",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e2a03a6b-5706-4355-ac24-94477606d4be",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:38:45.414866Z",
     "start_time": "2025-09-29T11:38:45.054342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: tech\n",
      "  mobile: 0.0115\n",
      "  software: 0.0106\n",
      "  users: 0.0104\n",
      "  microsoft: 0.0090\n",
      "  technology: 0.0090\n",
      "  people: 0.0087\n",
      "  computer: 0.0083\n",
      "  net: 0.0082\n",
      "  digital: 0.0080\n",
      "  broadband: 0.0080\n",
      "Category: sport\n",
      "  england: 0.0099\n",
      "  game: 0.0095\n",
      "  cup: 0.0089\n",
      "  win: 0.0085\n",
      "  match: 0.0085\n",
      "  injury: 0.0080\n",
      "  chelsea: 0.0079\n",
      "  club: 0.0078\n",
      "  team: 0.0074\n",
      "  season: 0.0073\n",
      "Category: business\n",
      "  bn: 0.0158\n",
      "  bank: 0.0097\n",
      "  growth: 0.0094\n",
      "  oil: 0.0090\n",
      "  economy: 0.0088\n",
      "  sales: 0.0085\n",
      "  shares: 0.0084\n",
      "  company: 0.0082\n",
      "  us: 0.0081\n",
      "  market: 0.0080\n",
      "Category: entertainment\n",
      "  film: 0.0249\n",
      "  best: 0.0125\n",
      "  awards: 0.0100\n",
      "  show: 0.0099\n",
      "  music: 0.0097\n",
      "  band: 0.0096\n",
      "  award: 0.0093\n",
      "  festival: 0.0091\n",
      "  album: 0.0088\n",
      "  actor: 0.0082\n",
      "Category: politics\n",
      "  labour: 0.0165\n",
      "  mr: 0.0164\n",
      "  election: 0.0142\n",
      "  blair: 0.0141\n",
      "  party: 0.0136\n",
      "  government: 0.0104\n",
      "  brown: 0.0097\n",
      "  howard: 0.0086\n",
      "  minister: 0.0083\n",
      "  tory: 0.0073\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# 2\n",
    "global_tf = np.sum(X_bow, axis=0) / np.sum(X_bow)\n",
    "high_tf_low_idf = []\n",
    "high_idf_low_tf = []\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    word_tf = global_tf[idx]\n",
    "    word_idf = idf_array[idx]\n",
    "\n",
    "    # High TF, Low IDF\n",
    "    if word_tf > np.median(global_tf) and word_idf < np.median(idf_array):\n",
    "        high_tf_low_idf.append((word, word_tf, word_idf))\n",
    "\n",
    "    # Low TF, High IDF\n",
    "    if word_tf < np.median(global_tf) and word_idf > np.median(idf_array):\n",
    "        high_idf_low_tf.append((word, word_tf, word_idf))\n",
    "\n",
    "# Sort and keep top 10\n",
    "high_tf_low_idf_sorted = sorted(high_tf_low_idf, key=lambda x: (x[1], -x[2]), reverse=True)[:10]\n",
    "high_idf_low_tf_sorted = sorted(high_idf_low_tf, key=lambda x: (x[2], -x[1]), reverse=True)[:10]\n",
    "\n",
    "# Results\n",
    "print(\"\\nWords with High TF, Low IDF:\")\n",
    "for word, tf_val, idf_val in high_tf_low_idf_sorted:\n",
    "    print(f\"  {word}: TF={tf_val:.4f}, IDF={idf_val:.4f}\")\n",
    "\n",
    "print(\"\\nWords with Low TF, High IDF:\")\n",
    "for word, tf_val, idf_val in high_idf_low_tf_sorted:\n",
    "    print(f\"  {word}: TF={tf_val:.4f}, IDF={idf_val:.4f}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "1fOKxTf-fGd5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f899ad76-8b93-433a-950f-a58708a7903c",
    "ExecuteTime": {
     "end_time": "2025-09-29T11:39:22.548995Z",
     "start_time": "2025-09-29T11:38:45.565877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with High TF, Low IDF:\n",
      "  said: TF=0.0149, IDF=0.1637\n",
      "  mr: TF=0.0062, IDF=1.0342\n",
      "  would: TF=0.0053, IDF=0.6626\n",
      "  year: TF=0.0047, IDF=0.6250\n",
      "  also: TF=0.0044, IDF=0.5639\n",
      "  people: TF=0.0042, IDF=1.0242\n",
      "  new: TF=0.0041, IDF=0.8179\n",
      "  us: TF=0.0040, IDF=0.9682\n",
      "  one: TF=0.0039, IDF=0.7284\n",
      "  could: TF=0.0031, IDF=0.9321\n",
      "\n",
      "Words with Low TF, High IDF:\n",
      "  aa: TF=0.0000, IDF=7.0144\n",
      "  aaltra: TF=0.0000, IDF=7.0144\n",
      "  aamir: TF=0.0000, IDF=7.0144\n",
      "  aan: TF=0.0000, IDF=7.0144\n",
      "  aara: TF=0.0000, IDF=7.0144\n",
      "  aarhus: TF=0.0000, IDF=7.0144\n",
      "  abate: TF=0.0000, IDF=7.0144\n",
      "  abatement: TF=0.0000, IDF=7.0144\n",
      "  abbot: TF=0.0000, IDF=7.0144\n",
      "  abbreviated: TF=0.0000, IDF=7.0144\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
